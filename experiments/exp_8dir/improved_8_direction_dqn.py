import numpy as np
import matplotlib.pyplot as plt
import heapq
import time
from collections import deque
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random

from irl_training.amr_path_planning_irl import AMRGridworld, ComputationalProfiler

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class ImprovedDQN8Directions(nn.Module):
    """ê°œì„ ëœ 8ë°©í–¥ DQN ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, input_size, hidden_size=256, output_size=8):
        super(ImprovedDQN8Directions, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_size),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_size),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, output_size)
        )
        
    def forward(self, x):
        return self.network(x)

class ImprovedDQN8DirectionsAgent:
    """ê°œì„ ëœ 8ë°©í–¥ DQN ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_size, action_size=8, lr=0.0005):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=50000)  # ë” í° ë©”ëª¨ë¦¬
        self.epsilon = 1.0
        self.epsilon_min = 0.05  # ë” ë†’ì€ ìµœì†Œê°’
        self.epsilon_decay = 0.9995  # ë” ëŠë¦° ê°ì†Œ
        self.gamma = 0.99
        self.learning_rate = lr
        self.batch_size = 64  # ë” í° ë°°ì¹˜
        
        # 8ë°©í–¥ ì´ë™ ì •ì˜
        self.directions = [
            (-1, 0),   # ìƒ
            (1, 0),    # í•˜
            (0, -1),   # ì¢Œ
            (0, 1),    # ìš°
            (-1, -1),  # ì¢Œìƒ
            (-1, 1),   # ìš°ìƒ
            (1, -1),   # ì¢Œí•˜
            (1, 1)     # ìš°í•˜
        ]
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.q_network = ImprovedDQN8Directions(state_size, 256, action_size).to(device)
        self.target_network = ImprovedDQN8Directions(state_size, 256, action_size).to(device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr, weight_decay=1e-5)
        
        self.update_target_counter = 0
        self.update_target_freq = 50  # ë” ìì£¼ ì—…ë°ì´íŠ¸
        
    def get_state_features(self, env, pos):
        """ê°œì„ ëœ ìƒíƒœ íŠ¹ì„± ì¶”ì¶œ"""
        x, y = pos
        goal_x, goal_y = env.goal_pos
        
        # ê¸°ë³¸ íŠ¹ì„± (4ê°œ)
        features = [
            x / env.grid_size,
            y / env.grid_size,
            goal_x / env.grid_size,
            goal_y / env.grid_size,
        ]
        
        # ê±°ë¦¬ ì •ë³´ (3ê°œ)
        dx = goal_x - x
        dy = goal_y - y
        manhattan_dist = abs(dx) + abs(dy)
        euclidean_dist = np.sqrt(dx**2 + dy**2)
        diagonal_dist = max(abs(dx), abs(dy)) + (1.414 - 1) * min(abs(dx), abs(dy))
        
        features.extend([
            manhattan_dist / (2 * env.grid_size),
            euclidean_dist / (np.sqrt(2) * env.grid_size),
            diagonal_dist / (np.sqrt(2) * env.grid_size),
        ])
        
        # ë°©í–¥ ì •ë³´ (2ê°œ)
        features.extend([
            dx / env.grid_size,
            dy / env.grid_size,
        ])
        
        # 8ë°©í–¥ ì£¼ë³€ ì¥ì• ë¬¼ ì •ë³´ (8ê°œ)
        for dx, dy in self.directions:
            nx, ny = x + dx, y + dy
            if 0 <= nx < env.grid_size and 0 <= ny < env.grid_size:
                features.append(float(env.grid[nx, ny] > 0))
            else:
                features.append(1.0)
        
        # ì¶”ê°€ íŠ¹ì„± (4ê°œ) - ì´ 21ê°œ
        features.extend([
            float(x == 0 or x == env.grid_size - 1),  # ê²½ê³„ ì—¬ë¶€
            float(y == 0 or y == env.grid_size - 1),  # ê²½ê³„ ì—¬ë¶€
            float(x == goal_x),  # x ìœ„ì¹˜ ì¼ì¹˜
            float(y == goal_y),  # y ìœ„ì¹˜ ì¼ì¹˜
        ])
        
        return np.array(features, dtype=np.float32)
    
    def act(self, state, training=True):
        """í–‰ë™ ì„ íƒ"""
        if training and np.random.random() <= self.epsilon:
            return random.randrange(self.action_size)
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        with torch.no_grad():
            q_values = self.q_network(state_tensor)
            return q_values.argmax().item()
    
    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        self.memory.append((state, action, reward, next_state, done))
    
    def replay(self):
        """ê²½í—˜ ì¬í”Œë ˆì´"""
        if len(self.memory) < self.batch_size:
            return
        
        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor([e[0] for e in batch]).to(device)
        actions = torch.LongTensor([e[1] for e in batch]).to(device)
        rewards = torch.FloatTensor([e[2] for e in batch]).to(device)
        next_states = torch.FloatTensor([e[3] for e in batch]).to(device)
        dones = torch.BoolTensor([e[4] for e in batch]).to(device)
        
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        self.update_target_counter += 1
        if self.update_target_counter % self.update_target_freq == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())

def calculate_improved_reward_8d(env, current_pos, next_pos, goal_pos, action_idx):
    """ê°œì„ ëœ 8ë°©í–¥ ë³´ìƒ í•¨ìˆ˜"""
    if next_pos == goal_pos:
        return 2000  # ë” í° ëª©í‘œ ë³´ìƒ
    
    if env.grid[next_pos[0], next_pos[1]] > 0:
        return -200  # ë” í° ì¥ì• ë¬¼ í˜ë„í‹°
    
    # ì´ë™ ë¹„ìš© ê³„ì‚°
    if action_idx < 4:  # ìƒí•˜ì¢Œìš°
        move_cost = 1.0
    else:  # ëŒ€ê°ì„ 
        move_cost = 1.414
    
    # ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ (8ë°©í–¥ ê³ ë ¤)
    old_dist = max(abs(current_pos[0] - goal_pos[0]), abs(current_pos[1] - goal_pos[1])) + \
               (1.414 - 1) * min(abs(current_pos[0] - goal_pos[0]), abs(current_pos[1] - goal_pos[1]))
    new_dist = max(abs(next_pos[0] - goal_pos[0]), abs(next_pos[1] - goal_pos[1])) + \
               (1.414 - 1) * min(abs(next_pos[0] - goal_pos[0]), abs(next_pos[1] - goal_pos[1]))
    
    if new_dist < old_dist:
        reward = 50  # ë” í° ê±°ë¦¬ ê°ì†Œ ë³´ìƒ
    elif new_dist == old_dist:
        reward = -1  # ë” ì‘ì€ í˜ë„í‹°
    else:
        reward = -20  # ë” í° ê±°ë¦¬ ì¦ê°€ í˜ë„í‹°
    
    # ëŒ€ê°ì„  ì´ë™ ë³´ìƒ
    if action_idx >= 4:  # ëŒ€ê°ì„  ì´ë™
        reward += 15  # ëŒ€ê°ì„  ì´ë™ ì¶”ê°€ ë³´ìƒ
    
    # ëª©í‘œ ë°©í–¥ìœ¼ë¡œì˜ ì´ë™ ë³´ìƒ
    goal_dx = goal_pos[0] - current_pos[0]
    goal_dy = goal_pos[1] - current_pos[1]
    move_dx = next_pos[0] - current_pos[0]
    move_dy = next_pos[1] - current_pos[1]
    
    # x ë°©í–¥ì´ ì˜¬ë°”ë¥´ë©´ ë³´ìƒ
    if goal_dx > 0 and move_dx > 0:  # ëª©í‘œê°€ ì˜¤ë¥¸ìª½ì— ìˆê³  ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™
        reward += 20
    elif goal_dx < 0 and move_dx < 0:  # ëª©í‘œê°€ ì™¼ìª½ì— ìˆê³  ì™¼ìª½ìœ¼ë¡œ ì´ë™
        reward += 20
    elif goal_dx == 0 and move_dx == 0:  # x ìœ„ì¹˜ê°€ ë§ìœ¼ë©´ ë³´ìƒ
        reward += 10
    
    # y ë°©í–¥ì´ ì˜¬ë°”ë¥´ë©´ ë³´ìƒ
    if goal_dy > 0 and move_dy > 0:  # ëª©í‘œê°€ ì•„ë˜ìª½ì— ìˆê³  ì•„ë˜ìª½ìœ¼ë¡œ ì´ë™
        reward += 20
    elif goal_dy < 0 and move_dy < 0:  # ëª©í‘œê°€ ìœ„ìª½ì— ìˆê³  ìœ„ìª½ìœ¼ë¡œ ì´ë™
        reward += 20
    elif goal_dy == 0 and move_dy == 0:  # y ìœ„ì¹˜ê°€ ë§ìœ¼ë©´ ë³´ìƒ
        reward += 10
    
    # ì˜ëª»ëœ ë°©í–¥ìœ¼ë¡œ ê°€ë©´ í° í˜ë„í‹°
    if (goal_dx > 0 and move_dx < 0) or (goal_dx < 0 and move_dx > 0):
        reward -= 30
    if (goal_dy > 0 and move_dy < 0) or (goal_dy < 0 and move_dy > 0):
        reward -= 30
    
    # ì´ë™ ë¹„ìš© í˜ë„í‹° (ë” ì‘ê²Œ)
    reward -= move_cost * 0.5
    
    return reward

def train_improved_dqn_8d(env, episodes=2000, max_steps=1000):
    """ê°œì„ ëœ 8ë°©í–¥ DQN í›ˆë ¨"""
    state_size = 21  # 4(ìœ„ì¹˜) + 3(ê±°ë¦¬) + 2(ë°©í–¥) + 8(ì£¼ë³€ì¥ì• ë¬¼) + 4(ì¶”ê°€íŠ¹ì„±)
    agent = ImprovedDQN8DirectionsAgent(state_size)
    
    print(f"Training Improved 8-direction DQN for {episodes} episodes...")
    training_start_time = time.time()
    scores = []
    success_count = 0
    
    for episode in range(episodes):
        current_pos = env.start_pos
        state = agent.get_state_features(env, current_pos)
        total_reward = 0
        visited_positions = set([current_pos])
        reached_goal = False
        
        for step in range(max_steps):
            action = agent.act(state, training=True)
            
            # 8ë°©í–¥ ì´ë™ ì‹¤í–‰
            dx, dy = agent.directions[action]
            next_pos = (current_pos[0] + dx, current_pos[1] + dy)
            
            # ê²½ê³„ ì²´í¬
            if (0 <= next_pos[0] < env.grid_size and 
                0 <= next_pos[1] < env.grid_size):
                
                reward = calculate_improved_reward_8d(env, current_pos, next_pos, env.goal_pos, action)
                
                # ì¬ë°©ë¬¸ í˜ë„í‹° (ë” ì‘ê²Œ)
                if next_pos in visited_positions:
                    reward -= 2
                else:
                    visited_positions.add(next_pos)
                
                next_state = agent.get_state_features(env, next_pos)
                done = (next_pos == env.goal_pos) or (step == max_steps - 1)
                
                if next_pos == env.goal_pos:
                    reached_goal = True
                
                agent.remember(state, action, reward, next_state, done)
                
                state = next_state
                current_pos = next_pos
                total_reward += reward
                
                if done:
                    break
            else:
                # ê²½ê³„ ë°–ìœ¼ë¡œ ë‚˜ê°€ë©´ í˜ë„í‹°
                reward = -100
                agent.remember(state, action, reward, state, False)
                total_reward += reward
        
        if reached_goal:
            success_count += 1
        
        if len(agent.memory) > agent.batch_size:
            agent.replay()
        
        scores.append(total_reward)
        
        if episode % 100 == 0:
            avg_score = np.mean(scores[-100:]) if len(scores) >= 100 else np.mean(scores)
            success_rate = success_count / (episode + 1) * 100
            print(f"Episode {episode}: Avg Score = {avg_score:.2f}, Epsilon = {agent.epsilon:.3f}, Success Rate = {success_rate:.1f}%")
    
    training_time = time.time() - training_start_time
    print(f"Improved 8-direction DQN training completed in {training_time:.2f}s")
    print(f"Final Success Rate: {success_count / episodes * 100:.1f}%")
    
    return agent, training_time

def improved_dqn_8d_path_planning(env, agent, profiler=None):
    """ê°œì„ ëœ 8ë°©í–¥ DQN ê²½ë¡œ ê³„íš"""
    if profiler:
        profiler.start_profiling()
    
    current_pos = env.start_pos
    path = [current_pos]
    max_steps = env.grid_size * 4  # ë” ê¸´ ìµœëŒ€ ìŠ¤í…
    visited = set([current_pos])
    
    for step in range(max_steps):
        if current_pos == env.goal_pos:
            print(f"Goal reached at step {step}")
            break
        
        state = agent.get_state_features(env, current_pos)
        action = agent.act(state, training=False)
        
        if profiler:
            profiler.increment_neural_inferences(1)
        
        dx, dy = agent.directions[action]
        next_pos = (current_pos[0] + dx, current_pos[1] + dy)
        
        if (0 <= next_pos[0] < env.grid_size and 
            0 <= next_pos[1] < env.grid_size and
            env.grid[next_pos[0], next_pos[1]] == 0):
            
            current_pos = next_pos
            path.append(current_pos)
            visited.add(current_pos)
        else:
            # ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë™ì´ë©´ ë‹¤ë¥¸ ë°©í–¥ ì‹œë„
            for alt_action in range(8):
                if alt_action != action:
                    dx, dy = agent.directions[alt_action]
                    alt_pos = (current_pos[0] + dx, current_pos[1] + dy)
                    if (0 <= alt_pos[0] < env.grid_size and 
                        0 <= alt_pos[1] < env.grid_size and
                        env.grid[alt_pos[0], alt_pos[1]] == 0):
                        current_pos = alt_pos
                        path.append(current_pos)
                        visited.add(current_pos)
                        break
            else:
                # ëª¨ë“  ë°©í–¥ì´ ë§‰í˜€ìˆìœ¼ë©´ ì¤‘ë‹¨
                print(f"Stuck at step {step}, no valid moves")
                break
    
    if profiler:
        profiler.end_profiling()
    
    return path, profiler.get_results() if profiler else {}

def test_improved_dqn_8d():
    """ê°œì„ ëœ 8ë°©í–¥ DQN í…ŒìŠ¤íŠ¸"""
    print("ğŸ§  Testing Improved 8-Direction DQN")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„±
    env = AMRGridworld(grid_size=20, obstacle_density=0.15, dynamic_obstacles=0)
    
    # ì‹œì‘ì ê³¼ ëª©í‘œì ì´ ì¥ì• ë¬¼ì— ë§‰íˆì§€ ì•Šë„ë¡ ë³´ì¥
    env.grid[env.start_pos[0], env.start_pos[1]] = 0
    env.grid[env.goal_pos[0], env.goal_pos[1]] = 0
    
    print(f"Environment: {env.grid_size}x{env.grid_size}, Start={env.start_pos}, Goal={env.goal_pos}")
    
    # ê°œì„ ëœ DQN í›ˆë ¨
    agent, training_time = train_improved_dqn_8d(env, episodes=2000)
    
    # ê²½ë¡œ ê³„íš
    profiler = ComputationalProfiler()
    start_time = time.time()
    path, stats = improved_dqn_8d_path_planning(env, agent, profiler)
    inference_time = time.time() - start_time
    
    print(f"\nImproved DQN 8D Results:")
    print(f"  Path Length: {len(path)}")
    print(f"  Training Time: {training_time:.2f}s")
    print(f"  Inference Time: {inference_time:.4f}s")
    print(f"  Reached Goal: {path[-1] == env.goal_pos if path else False}")
    
    # ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # ê·¸ë¦¬ë“œ í‘œì‹œ
    for i in range(env.grid_size + 1):
        ax.axhline(y=i, color='gray', alpha=0.3)
        ax.axvline(x=i, color='gray', alpha=0.3)
    
    # ì¥ì• ë¬¼ í‘œì‹œ
    for i in range(env.grid_size):
        for j in range(env.grid_size):
            if env.grid[i, j] > 0:
                ax.add_patch(plt.Rectangle((j, env.grid_size-1-i), 1, 1, 
                                        facecolor='black', alpha=0.7))
    
    # ì‹œì‘ì ê³¼ ëª©í‘œì 
    start_x, start_y = env.start_pos[1], env.grid_size-1-env.start_pos[0]
    goal_x, goal_y = env.goal_pos[1], env.grid_size-1-env.goal_pos[0]
    
    ax.plot(start_x + 0.5, start_y + 0.5, 'go', markersize=15, label='Start')
    ax.plot(goal_x + 0.5, goal_y + 0.5, 'ro', markersize=15, label='Goal')
    
    # ê²½ë¡œ í‘œì‹œ
    if path and len(path) > 1:
        x = [p[1] + 0.5 for p in path]
        y = [env.grid_size-1-p[0] + 0.5 for p in path]
        ax.plot(x, y, 'b-', linewidth=3, label='Improved DQN 8D', alpha=0.8)
        ax.plot(x, y, 'bo', markersize=6)
    
    ax.set_xlim(0, env.grid_size)
    ax.set_ylim(0, env.grid_size)
    ax.set_aspect('equal')
    ax.set_title('Improved 8-Direction DQN Path Planning')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.savefig('improved_8_direction_dqn.png', dpi=300, bbox_inches='tight')
    print("Visualization saved as: improved_8_direction_dqn.png")
    plt.close()

if __name__ == "__main__":
    test_improved_dqn_8d()
